{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIIdiJpPojFkl6ckrgs+3J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Requenamar3/DeepLearning/blob/main/Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQBumESC22aN"
      },
      "outputs": [],
      "source": [
        "#import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Q2jfVqefDQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#connect to dataset\n",
        "url = 'https://raw.githubusercontent.com/fenago/datasets/main/mushrooms.csv'\n",
        "df = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "3IMwN0Db3RXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#inspect data set\n",
        "df.sample(5)"
      ],
      "metadata": {
        "id": "rBGcM_Ih3eH5",
        "outputId": "f47f5273-0776-49e8-d302-89ad6d6dcb98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     class cap-shape cap-surface cap-color bruises odor gill-attachment  \\\n",
              "864      e         x           y         y       t    a               f   \n",
              "6385     p         f           y         e       f    f               f   \n",
              "5243     e         k           s         b       t    n               f   \n",
              "3265     p         x           f         g       f    f               f   \n",
              "2342     e         x           f         e       t    n               f   \n",
              "\n",
              "     gill-spacing gill-size gill-color  ... stalk-surface-below-ring  \\\n",
              "864             c         b          w  ...                        s   \n",
              "6385            c         n          b  ...                        s   \n",
              "5243            c         b          e  ...                        s   \n",
              "3265            c         b          h  ...                        k   \n",
              "2342            c         b          u  ...                        s   \n",
              "\n",
              "     stalk-color-above-ring stalk-color-below-ring veil-type veil-color  \\\n",
              "864                       w                      w         p          w   \n",
              "6385                      p                      p         p          w   \n",
              "5243                      w                      e         p          w   \n",
              "3265                      b                      n         p          w   \n",
              "2342                      p                      w         p          w   \n",
              "\n",
              "     ring-number ring-type spore-print-color population habitat  \n",
              "864            o         p                 n          s       m  \n",
              "6385           o         e                 w          v       l  \n",
              "5243           t         e                 w          c       w  \n",
              "3265           o         l                 h          y       p  \n",
              "2342           o         p                 n          v       d  \n",
              "\n",
              "[5 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-244f18b5-479d-4d32-8ec9-e102d6395910\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>cap-shape</th>\n",
              "      <th>cap-surface</th>\n",
              "      <th>cap-color</th>\n",
              "      <th>bruises</th>\n",
              "      <th>odor</th>\n",
              "      <th>gill-attachment</th>\n",
              "      <th>gill-spacing</th>\n",
              "      <th>gill-size</th>\n",
              "      <th>gill-color</th>\n",
              "      <th>...</th>\n",
              "      <th>stalk-surface-below-ring</th>\n",
              "      <th>stalk-color-above-ring</th>\n",
              "      <th>stalk-color-below-ring</th>\n",
              "      <th>veil-type</th>\n",
              "      <th>veil-color</th>\n",
              "      <th>ring-number</th>\n",
              "      <th>ring-type</th>\n",
              "      <th>spore-print-color</th>\n",
              "      <th>population</th>\n",
              "      <th>habitat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>864</th>\n",
              "      <td>e</td>\n",
              "      <td>x</td>\n",
              "      <td>y</td>\n",
              "      <td>y</td>\n",
              "      <td>t</td>\n",
              "      <td>a</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>b</td>\n",
              "      <td>w</td>\n",
              "      <td>...</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>p</td>\n",
              "      <td>n</td>\n",
              "      <td>s</td>\n",
              "      <td>m</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6385</th>\n",
              "      <td>p</td>\n",
              "      <td>f</td>\n",
              "      <td>y</td>\n",
              "      <td>e</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>n</td>\n",
              "      <td>b</td>\n",
              "      <td>...</td>\n",
              "      <td>s</td>\n",
              "      <td>p</td>\n",
              "      <td>p</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>e</td>\n",
              "      <td>w</td>\n",
              "      <td>v</td>\n",
              "      <td>l</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5243</th>\n",
              "      <td>e</td>\n",
              "      <td>k</td>\n",
              "      <td>s</td>\n",
              "      <td>b</td>\n",
              "      <td>t</td>\n",
              "      <td>n</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>b</td>\n",
              "      <td>e</td>\n",
              "      <td>...</td>\n",
              "      <td>s</td>\n",
              "      <td>w</td>\n",
              "      <td>e</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>t</td>\n",
              "      <td>e</td>\n",
              "      <td>w</td>\n",
              "      <td>c</td>\n",
              "      <td>w</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3265</th>\n",
              "      <td>p</td>\n",
              "      <td>x</td>\n",
              "      <td>f</td>\n",
              "      <td>g</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>b</td>\n",
              "      <td>h</td>\n",
              "      <td>...</td>\n",
              "      <td>k</td>\n",
              "      <td>b</td>\n",
              "      <td>n</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>l</td>\n",
              "      <td>h</td>\n",
              "      <td>y</td>\n",
              "      <td>p</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2342</th>\n",
              "      <td>e</td>\n",
              "      <td>x</td>\n",
              "      <td>f</td>\n",
              "      <td>e</td>\n",
              "      <td>t</td>\n",
              "      <td>n</td>\n",
              "      <td>f</td>\n",
              "      <td>c</td>\n",
              "      <td>b</td>\n",
              "      <td>u</td>\n",
              "      <td>...</td>\n",
              "      <td>s</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>p</td>\n",
              "      <td>w</td>\n",
              "      <td>o</td>\n",
              "      <td>p</td>\n",
              "      <td>n</td>\n",
              "      <td>v</td>\n",
              "      <td>d</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 23 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-244f18b5-479d-4d32-8ec9-e102d6395910')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-244f18b5-479d-4d32-8ec9-e102d6395910 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-244f18b5-479d-4d32-8ec9-e102d6395910');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert categorical variables into binary or dummy variables\n",
        "df_encoded = pd.get_dummies(df, drop_first=True)"
      ],
      "metadata": {
        "id": "zI6WjcFg3r9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting data set for training\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = df_encoded.drop('class_p', axis=1)\n",
        "y = df_encoded['class_p']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "xocYOb-T8IZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "X_train: Training set features\n",
        "\n",
        "X_test: Testing set features\n",
        "\n",
        "y_train: Training set target variable\n",
        "\n",
        "y_test: Testing set target variable\n",
        "\n",
        "test_size=0.2: 20% of the data is used for testing and 80% for training\n",
        "\n",
        "random_state=42: Sets the random seed to 42 for reproducibility of the split\n"
      ],
      "metadata": {
        "id": "IS7a67OLF-GL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the samples in each set\n",
        "training_samples = X_train.shape[0]\n",
        "testing_samples = X_test.shape[0]\n",
        "\n",
        "print(\"Number of samples in the training set:\", training_samples)\n",
        "print(\"Number of samples in the testing set:\", testing_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ar8teuo8VDA",
        "outputId": "2a17475d-23ac-48c8-b9a0-505ea97b83c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in the training set: 6499\n",
            "Number of samples in the testing set: 1625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu'),  # First hidden layer with 64 neurons and ReLU activation\n",
        "    tf.keras.layers.Dense(128, activation='relu'),  # Second hidden layer with 128 neurons and ReLU activation\n",
        "    tf.keras.layers.Dense(128, activation='relu'),  # Third hidden layer with 128 neurons and ReLU activation\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer with 1 neuron and Sigmoid activation\n",
        "])\n",
        "\n",
        "# Compile the model with the specified loss, optimizer, and metrics\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),  # Binary cross-entropy loss function\n",
        "    optimizer=tf.keras.optimizers.Adam(lr=0.001),  # Adam optimizer with learning rate 0.001\n",
        "    metrics=[\n",
        "        tf.keras.metrics.BinaryAccuracy(name='accuracy')  # Binary accuracy metric for evaluation\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train the model on the training data\n",
        "history = model.fit(X_train, y_train, epochs=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fr5fvcrvFdEI",
        "outputId": "d36f1ed5-e6af-424f-fcd1-38760faccc0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "204/204 [==============================] - 2s 3ms/step - loss: 0.0798 - accuracy: 0.9757\n",
            "Epoch 2/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 9.0161e-04 - accuracy: 0.9998\n",
            "Epoch 3/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.3031e-04 - accuracy: 1.0000\n",
            "Epoch 4/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 5.2258e-05 - accuracy: 1.0000\n",
            "Epoch 5/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.0880e-05 - accuracy: 1.0000\n",
            "Epoch 6/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.0644e-05 - accuracy: 1.0000\n",
            "Epoch 7/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 6.4084e-06 - accuracy: 1.0000\n",
            "Epoch 8/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.2098e-06 - accuracy: 1.0000\n",
            "Epoch 9/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.9808e-06 - accuracy: 1.0000\n",
            "Epoch 10/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.1932e-06 - accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.6465e-06 - accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.3008e-06 - accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.0295e-06 - accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 8.3488e-07 - accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 6.8475e-07 - accuracy: 1.0000\n",
            "Epoch 16/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.7156e-07 - accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.8014e-07 - accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.0490e-07 - accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.4545e-07 - accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.9612e-07 - accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.5467e-07 - accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.2010e-07 - accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.9089e-07 - accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.6607e-07 - accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.4538e-07 - accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.2769e-07 - accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.1186e-07 - accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 9.8817e-08 - accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 8.7396e-08 - accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 7.7120e-08 - accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 6.8366e-08 - accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 6.0876e-08 - accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.4020e-08 - accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.8238e-08 - accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.2938e-08 - accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.8295e-08 - accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.4231e-08 - accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.0625e-08 - accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.7450e-08 - accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.4707e-08 - accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.2065e-08 - accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.9869e-08 - accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.7881e-08 - accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.6070e-08 - accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.4528e-08 - accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.3062e-08 - accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.1761e-08 - accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.0575e-08 - accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 9.6136e-09 - accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 8.7194e-09 - accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.8380e-09 - accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.0913e-09 - accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 6.4139e-09 - accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 5.8414e-09 - accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 5.3022e-09 - accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 4.7965e-09 - accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 4.3744e-09 - accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 3.9988e-09 - accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.6362e-09 - accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.3025e-09 - accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.0344e-09 - accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.7834e-09 - accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.5345e-09 - accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.3211e-09 - accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.1245e-09 - accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.9550e-09 - accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.7909e-09 - accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.6457e-09 - accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.5147e-09 - accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.3955e-09 - accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.2937e-09 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.2037e-09 - accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.1188e-09 - accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.0448e-09 - accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 9.7450e-10 - accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 9.0858e-10 - accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 8.5679e-10 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.9033e-10 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.5297e-10 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 7.1007e-10 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 6.5008e-10 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 6.3281e-10 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 6.0149e-10 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 5.4782e-10 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.2811e-10 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.9293e-10 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.7650e-10 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.5971e-10 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.3572e-10 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.2179e-10 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.7957e-10 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.7821e-10 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.5096e-10 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.4208e-10 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.2594e-10 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.0565e-10 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.9486e-10 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.9098e-10 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.7913e-10 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.7182e-10 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#making predictions\n",
        "predictions = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAwICqPRD2vI",
        "outputId": "b719316c-07b5-4e42-85ee-01247b7d797c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51/51 [==============================] - 0s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_classes_class = [\n",
        "    'p' if prob > 0.5 else 'e' for prob in np.ravel(predictions)\n",
        "]\n",
        "# prediction_classes_class: Binary predictions converted to class labels ('p' for poisonous, 'e' for edible)\n",
        "# predictions: Predicted values for the testing set based on the trained model\n",
        "# np.ravel(predictions): Flattened predictions array for easier iteration\n",
        "\n",
        "prediction_classes = [\n",
        "    1 if prob > 0.5 else 0 for prob in np.ravel(predictions)\n",
        "]\n",
        "# prediction_classes: Binary predictions (0 or 1) based on the predicted probabilities\n",
        "# If the probability (prob) is greater than 0.5, the prediction is assigned 1 (indicating 'p' or poisonous),\n",
        "# otherwise, it is assigned 0 (indicating 'e' or edible)\n",
        "\n",
        "prediction_classes\n",
        "# prediction_classes: Final binary predictions (0 or 1) for the testing set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "io1FaWWiFbyn",
        "outputId": "492ce0d7-4b8d-4d77-d59f-797aaaa7caa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Compute the confusion matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, prediction_classes)\n",
        "# cm: Confusion matrix computed using the true target values (y_test) and the predicted classes (prediction_classes)\n",
        "# y_test: True target values for the testing set\n",
        "# prediction_classes: Predicted classes (binary predictions) for the testing set\n",
        "\n",
        "print(cm)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMA8hmb9EwZS",
        "outputId": "31c3448d-8750-4a81-a7c2-2f9f5c749d95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[843   0]\n",
            " [  0 782]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Compute the accuracy score\n",
        "accuracy = accuracy_score(y_test, prediction_classes)\n",
        "# accuracy: Accuracy score computed using the true target values (y_test) and the predicted classes (prediction_classes)\n",
        "# y_test: True target values for the testing set\n",
        "# prediction_classes: Predicted classes (binary predictions) for the testing set\n",
        "\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "# Print the accuracy score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKazzQLfITPG",
        "outputId": "ab0b53ef-53b3-44e5-92f8-ab5db75448f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#changing the learning rate\n",
        "# Create the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(lr=0.0001),  # Decreasing learning rate to 0.0001\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy')]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1t5xucwROZw",
        "outputId": "fca411eb-6493-48e9-a8e4-dbb7ff2151ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "204/204 [==============================] - 2s 2ms/step - loss: 0.0812 - accuracy: 0.9715\n",
            "Epoch 2/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 5.1734e-04 - accuracy: 1.0000\n",
            "Epoch 3/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.3151e-04 - accuracy: 1.0000\n",
            "Epoch 4/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 6.8339e-05 - accuracy: 1.0000\n",
            "Epoch 5/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 3.7888e-05 - accuracy: 1.0000\n",
            "Epoch 6/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 2.4691e-05 - accuracy: 1.0000\n",
            "Epoch 7/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.7293e-05 - accuracy: 1.0000\n",
            "Epoch 8/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.2466e-05 - accuracy: 1.0000\n",
            "Epoch 9/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 9.2043e-06 - accuracy: 1.0000\n",
            "Epoch 10/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.1701e-06 - accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.5976e-06 - accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.5055e-06 - accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.6314e-06 - accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.9616e-06 - accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.4396e-06 - accuracy: 1.0000\n",
            "Epoch 16/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.0357e-06 - accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "204/204 [==============================] - 1s 6ms/step - loss: 1.7195e-06 - accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.4584e-06 - accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.2340e-06 - accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.0594e-06 - accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 9.1611e-07 - accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.8663e-07 - accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 6.8130e-07 - accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.8752e-07 - accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.0492e-07 - accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.4374e-07 - accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.8450e-07 - accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 3.3708e-07 - accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 2.9783e-07 - accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 2.5931e-07 - accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 2.2841e-07 - accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.9976e-07 - accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.7625e-07 - accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.5630e-07 - accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.3765e-07 - accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.2166e-07 - accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.0701e-07 - accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 9.4298e-08 - accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 8.3541e-08 - accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.4557e-08 - accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 6.5933e-08 - accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.8345e-08 - accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.1927e-08 - accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.5551e-08 - accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.0673e-08 - accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.5995e-08 - accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.1845e-08 - accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.8314e-08 - accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.5120e-08 - accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.2512e-08 - accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.9909e-08 - accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.7808e-08 - accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.5849e-08 - accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.4211e-08 - accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.2695e-08 - accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.1456e-08 - accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.0238e-08 - accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 9.2318e-09 - accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 8.3152e-09 - accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 7.5339e-09 - accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 6.8130e-09 - accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 6.1986e-09 - accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "204/204 [==============================] - 1s 6ms/step - loss: 5.5664e-09 - accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 5.0797e-09 - accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "204/204 [==============================] - 1s 6ms/step - loss: 4.6244e-09 - accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 4.2345e-09 - accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 3.8391e-09 - accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 3.5110e-09 - accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.2059e-09 - accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 2.9274e-09 - accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 2.6904e-09 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 2.4550e-09 - accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 2.2609e-09 - accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 2.0770e-09 - accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.9299e-09 - accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.7752e-09 - accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.6032e-09 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.5105e-09 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.4015e-09 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.3121e-09 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.2148e-09 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.1566e-09 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.0787e-09 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 9.9657e-10 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 9.6949e-10 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 8.6284e-10 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 8.1562e-10 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 8.0717e-10 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.4423e-10 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.0436e-10 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 6.4686e-10 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 6.5541e-10 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.8668e-10 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.7648e-10 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.5752e-10 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.1210e-10 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.9776e-10 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 4.6564e-10 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 4.4473e-10 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 4.1633e-10 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#changing number of layers\n",
        "\n",
        "# Create the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'), # adding another layer\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
        "    metrics=[\n",
        "        tf.keras.metrics.BinaryAccuracy(name='accuracy')]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VS5JZSJ5Rj5v",
        "outputId": "2bcd126a-8fad-4a94-d15d-4b9bd41caac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "204/204 [==============================] - 2s 2ms/step - loss: 0.0701 - accuracy: 0.9743\n",
            "Epoch 2/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.5257e-04 - accuracy: 1.0000\n",
            "Epoch 3/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.1817e-05 - accuracy: 1.0000\n",
            "Epoch 4/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.3816e-05 - accuracy: 1.0000\n",
            "Epoch 5/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.9350e-05 - accuracy: 1.0000\n",
            "Epoch 6/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.1997e-05 - accuracy: 1.0000\n",
            "Epoch 7/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 8.2041e-06 - accuracy: 1.0000\n",
            "Epoch 8/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.6819e-06 - accuracy: 1.0000\n",
            "Epoch 9/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.9668e-06 - accuracy: 1.0000\n",
            "Epoch 10/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.0192e-06 - accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.2595e-06 - accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.7200e-06 - accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.2923e-06 - accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 9.4032e-07 - accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 6.8497e-07 - accuracy: 1.0000\n",
            "Epoch 16/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 4.9658e-07 - accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 3.7368e-07 - accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 2.9391e-07 - accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 2.2651e-07 - accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 1.8113e-07 - accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 1.4572e-07 - accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 1.2059e-07 - accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "204/204 [==============================] - 1s 7ms/step - loss: 1.0128e-07 - accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 8.4637e-08 - accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 7.2221e-08 - accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 6.2472e-08 - accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.4251e-08 - accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.6291e-08 - accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 4.1149e-08 - accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 3.5901e-08 - accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 3.2117e-08 - accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 2.8656e-08 - accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 2.5450e-08 - accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 2.2688e-08 - accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 2.0074e-08 - accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 1.8237e-08 - accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.6347e-08 - accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.4633e-08 - accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.3135e-08 - accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.2082e-08 - accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.0926e-08 - accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 9.8162e-09 - accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 8.9698e-09 - accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 8.1414e-09 - accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.3616e-09 - accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 6.7133e-09 - accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 6.1890e-09 - accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.5521e-09 - accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 5.1658e-09 - accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 4.7299e-09 - accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 4.3101e-09 - accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 3.9505e-09 - accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.5683e-09 - accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.3493e-09 - accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 3.0855e-09 - accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 2.7857e-09 - accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 2.6078e-09 - accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 2.4208e-09 - accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 2.2435e-09 - accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 2.0416e-09 - accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 1.9284e-09 - accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.7649e-09 - accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.6125e-09 - accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.5081e-09 - accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.3905e-09 - accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 1.3019e-09 - accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.2005e-09 - accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.1241e-09 - accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 1.0528e-09 - accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 9.6751e-10 - accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 9.0952e-10 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 8.6516e-10 - accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.9181e-10 - accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.5583e-10 - accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 7.1297e-10 - accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 6.6718e-10 - accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 6.2588e-10 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.8629e-10 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 5.5916e-10 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 5.3105e-10 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 4.8619e-10 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 4.8265e-10 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 4.3943e-10 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 4.1544e-10 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 3.9884e-10 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.7159e-10 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.5661e-10 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.5028e-10 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.3061e-10 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 3.2123e-10 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.9798e-10 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.8749e-10 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.8065e-10 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.7387e-10 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.5936e-10 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.4764e-10 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.3999e-10 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.3514e-10 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.2640e-10 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "204/204 [==============================] - 0s 2ms/step - loss: 2.1377e-10 - accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define the model with L1 regularization\n",
        "model_l1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1(0.001)),\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1(0.001)),\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1(0.001)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_l1.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy')]\n",
        ")\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    verbose=1,\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train the model with early stopping\n",
        "history_l1 = model_l1.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stop]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xWGoH3HaOoO",
        "outputId": "f9de2154-39be-4b23-a498-46fa6d320877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "204/204 [==============================] - 2s 3ms/step - loss: 1.3033 - accuracy: 0.9665 - val_loss: 0.4597 - val_accuracy: 0.9988\n",
            "Epoch 2/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.3122 - accuracy: 0.9986 - val_loss: 0.2222 - val_accuracy: 0.9994\n",
            "Epoch 3/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1778 - accuracy: 0.9988 - val_loss: 0.1421 - val_accuracy: 0.9994\n",
            "Epoch 4/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.1224 - accuracy: 0.9988 - val_loss: 0.1041 - val_accuracy: 0.9994\n",
            "Epoch 5/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0933 - accuracy: 0.9992 - val_loss: 0.0826 - val_accuracy: 0.9994\n",
            "Epoch 6/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0749 - accuracy: 0.9989 - val_loss: 0.0720 - val_accuracy: 1.0000\n",
            "Epoch 7/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0631 - accuracy: 0.9994 - val_loss: 0.0577 - val_accuracy: 0.9994\n",
            "Epoch 8/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0551 - accuracy: 0.9992 - val_loss: 0.0512 - val_accuracy: 1.0000\n",
            "Epoch 9/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0491 - accuracy: 0.9997 - val_loss: 0.0460 - val_accuracy: 0.9994\n",
            "Epoch 10/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0446 - accuracy: 0.9992 - val_loss: 0.0450 - val_accuracy: 1.0000\n",
            "Epoch 11/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0408 - accuracy: 1.0000 - val_loss: 0.0393 - val_accuracy: 1.0000\n",
            "Epoch 12/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0380 - accuracy: 0.9997 - val_loss: 0.0372 - val_accuracy: 1.0000\n",
            "Epoch 13/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0355 - accuracy: 0.9998 - val_loss: 0.0341 - val_accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0334 - accuracy: 0.9998 - val_loss: 0.0326 - val_accuracy: 1.0000\n",
            "Epoch 15/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0316 - accuracy: 0.9998 - val_loss: 0.0303 - val_accuracy: 1.0000\n",
            "Epoch 16/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0298 - accuracy: 0.9997 - val_loss: 0.0289 - val_accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0285 - accuracy: 1.0000 - val_loss: 0.0275 - val_accuracy: 1.0000\n",
            "Epoch 18/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0271 - accuracy: 1.0000 - val_loss: 0.0262 - val_accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 0.0252 - val_accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0251 - accuracy: 1.0000 - val_loss: 0.0247 - val_accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 1.0000\n",
            "Epoch 23/100\n",
            "204/204 [==============================] - 1s 2ms/step - loss: 0.0241 - accuracy: 0.9998 - val_loss: 0.0231 - val_accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0226 - accuracy: 0.9998 - val_loss: 0.0225 - val_accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0219 - accuracy: 1.0000 - val_loss: 0.0215 - val_accuracy: 1.0000\n",
            "Epoch 27/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0215 - accuracy: 1.0000 - val_loss: 0.0211 - val_accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0210 - accuracy: 1.0000 - val_loss: 0.0212 - val_accuracy: 1.0000\n",
            "Epoch 29/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0206 - accuracy: 1.0000 - val_loss: 0.0205 - val_accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0202 - accuracy: 1.0000 - val_loss: 0.0199 - val_accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0199 - accuracy: 1.0000 - val_loss: 0.0195 - val_accuracy: 1.0000\n",
            "Epoch 32/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 0.0192 - val_accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0193 - accuracy: 1.0000 - val_loss: 0.0191 - val_accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 1.0000\n",
            "Epoch 35/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 0.0186 - val_accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 0.0183 - val_accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.0180 - val_accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0181 - accuracy: 1.0000 - val_loss: 0.0180 - val_accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.0178 - val_accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0177 - accuracy: 1.0000 - val_loss: 0.0173 - val_accuracy: 1.0000\n",
            "Epoch 41/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.0170 - val_accuracy: 1.0000\n",
            "Epoch 42/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0171 - accuracy: 1.0000 - val_loss: 0.0169 - val_accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0170 - accuracy: 1.0000 - val_loss: 0.0168 - val_accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0166 - accuracy: 1.0000 - val_loss: 0.0164 - val_accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.0162 - val_accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0161 - accuracy: 1.0000 - val_loss: 0.0159 - val_accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.0162 - val_accuracy: 1.0000\n",
            "Epoch 48/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0159 - accuracy: 1.0000 - val_loss: 0.0156 - val_accuracy: 1.0000\n",
            "Epoch 49/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0158 - accuracy: 1.0000 - val_loss: 0.0157 - val_accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0155 - accuracy: 1.0000 - val_loss: 0.0152 - val_accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.0152 - val_accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.0150 - val_accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.0149 - val_accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.0148 - val_accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.0145 - val_accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.0145 - val_accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.0146 - val_accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.0142 - val_accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.0142 - val_accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.0142 - val_accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.0140 - val_accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.0140 - val_accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.0138 - val_accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.0138 - val_accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.0138 - val_accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.0135 - val_accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0132 - val_accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.0132 - val_accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.0129 - val_accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.0128 - val_accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.0128 - val_accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0129 - val_accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.0127 - val_accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.0129 - val_accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.0126 - val_accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0127 - val_accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.0126 - val_accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.0128 - val_accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0127 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.0126 - val_accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.0127 - val_accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Add dropout layers to our model to further reduce overfitting.Dropout layers randomly disable a percentage of neurons during training, forcing the remaining neurons to learn more robust features. We can add dropout layers after each of our dense layers"
      ],
      "metadata": {
        "id": "v2OslQULkN8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Create the model with L1 regularization and dropout layers\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l1(0.001)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1(0.001)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l1(0.001)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy')]\n",
        ")\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    verbose=1,\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stop]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkNJ6bS7djhq",
        "outputId": "8c62c9a8-70e9-4177-ed5a-a5be1d866017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "204/204 [==============================] - 2s 4ms/step - loss: 1.8124 - accuracy: 0.8687 - val_loss: 0.8641 - val_accuracy: 0.9877\n",
            "Epoch 2/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.6193 - accuracy: 0.9837 - val_loss: 0.3949 - val_accuracy: 0.9975\n",
            "Epoch 3/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.3446 - accuracy: 0.9906 - val_loss: 0.2605 - val_accuracy: 0.9988\n",
            "Epoch 4/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.2591 - accuracy: 0.9915 - val_loss: 0.2067 - val_accuracy: 0.9988\n",
            "Epoch 5/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.2203 - accuracy: 0.9931 - val_loss: 0.1794 - val_accuracy: 0.9988\n",
            "Epoch 6/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.1921 - accuracy: 0.9946 - val_loss: 0.1632 - val_accuracy: 0.9982\n",
            "Epoch 7/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1769 - accuracy: 0.9943 - val_loss: 0.1461 - val_accuracy: 0.9994\n",
            "Epoch 8/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1624 - accuracy: 0.9958 - val_loss: 0.1358 - val_accuracy: 0.9994\n",
            "Epoch 9/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1528 - accuracy: 0.9952 - val_loss: 0.1273 - val_accuracy: 0.9988\n",
            "Epoch 10/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.1486 - accuracy: 0.9949 - val_loss: 0.1280 - val_accuracy: 0.9994\n",
            "Epoch 11/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 0.1432 - accuracy: 0.9951 - val_loss: 0.1190 - val_accuracy: 0.9994\n",
            "Epoch 12/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.1396 - accuracy: 0.9954 - val_loss: 0.1144 - val_accuracy: 0.9994\n",
            "Epoch 13/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 0.1326 - accuracy: 0.9954 - val_loss: 0.1127 - val_accuracy: 1.0000\n",
            "Epoch 14/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1317 - accuracy: 0.9958 - val_loss: 0.1084 - val_accuracy: 0.9994\n",
            "Epoch 15/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1270 - accuracy: 0.9958 - val_loss: 0.1055 - val_accuracy: 0.9994\n",
            "Epoch 16/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1231 - accuracy: 0.9957 - val_loss: 0.1049 - val_accuracy: 1.0000\n",
            "Epoch 17/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1203 - accuracy: 0.9966 - val_loss: 0.1026 - val_accuracy: 0.9994\n",
            "Epoch 18/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1214 - accuracy: 0.9955 - val_loss: 0.1022 - val_accuracy: 1.0000\n",
            "Epoch 19/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1187 - accuracy: 0.9965 - val_loss: 0.0995 - val_accuracy: 1.0000\n",
            "Epoch 20/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1158 - accuracy: 0.9963 - val_loss: 0.0979 - val_accuracy: 1.0000\n",
            "Epoch 21/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1137 - accuracy: 0.9968 - val_loss: 0.0958 - val_accuracy: 1.0000\n",
            "Epoch 22/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1126 - accuracy: 0.9958 - val_loss: 0.0958 - val_accuracy: 0.9994\n",
            "Epoch 23/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1119 - accuracy: 0.9963 - val_loss: 0.0943 - val_accuracy: 1.0000\n",
            "Epoch 24/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1103 - accuracy: 0.9958 - val_loss: 0.0923 - val_accuracy: 1.0000\n",
            "Epoch 25/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1092 - accuracy: 0.9957 - val_loss: 0.0916 - val_accuracy: 1.0000\n",
            "Epoch 26/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1072 - accuracy: 0.9960 - val_loss: 0.0904 - val_accuracy: 0.9994\n",
            "Epoch 27/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1099 - accuracy: 0.9949 - val_loss: 0.0918 - val_accuracy: 1.0000\n",
            "Epoch 28/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1066 - accuracy: 0.9969 - val_loss: 0.0897 - val_accuracy: 0.9994\n",
            "Epoch 29/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.1052 - accuracy: 0.9965 - val_loss: 0.0882 - val_accuracy: 1.0000\n",
            "Epoch 30/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.1044 - accuracy: 0.9958 - val_loss: 0.0901 - val_accuracy: 1.0000\n",
            "Epoch 31/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.1029 - accuracy: 0.9965 - val_loss: 0.0857 - val_accuracy: 0.9994\n",
            "Epoch 32/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.1038 - accuracy: 0.9955 - val_loss: 0.0851 - val_accuracy: 1.0000\n",
            "Epoch 33/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1011 - accuracy: 0.9966 - val_loss: 0.0838 - val_accuracy: 1.0000\n",
            "Epoch 34/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0998 - accuracy: 0.9966 - val_loss: 0.0817 - val_accuracy: 0.9994\n",
            "Epoch 35/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1012 - accuracy: 0.9962 - val_loss: 0.0841 - val_accuracy: 1.0000\n",
            "Epoch 36/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1002 - accuracy: 0.9955 - val_loss: 0.0835 - val_accuracy: 1.0000\n",
            "Epoch 37/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0978 - accuracy: 0.9965 - val_loss: 0.0824 - val_accuracy: 1.0000\n",
            "Epoch 38/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0999 - accuracy: 0.9966 - val_loss: 0.0826 - val_accuracy: 1.0000\n",
            "Epoch 39/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0960 - accuracy: 0.9971 - val_loss: 0.0807 - val_accuracy: 1.0000\n",
            "Epoch 40/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.1012 - accuracy: 0.9945 - val_loss: 0.0813 - val_accuracy: 0.9994\n",
            "Epoch 41/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0961 - accuracy: 0.9954 - val_loss: 0.0810 - val_accuracy: 0.9994\n",
            "Epoch 42/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0958 - accuracy: 0.9962 - val_loss: 0.0787 - val_accuracy: 1.0000\n",
            "Epoch 43/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0903 - accuracy: 0.9977 - val_loss: 0.0773 - val_accuracy: 1.0000\n",
            "Epoch 44/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0933 - accuracy: 0.9966 - val_loss: 0.0772 - val_accuracy: 1.0000\n",
            "Epoch 45/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0938 - accuracy: 0.9971 - val_loss: 0.0763 - val_accuracy: 1.0000\n",
            "Epoch 46/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0909 - accuracy: 0.9960 - val_loss: 0.0758 - val_accuracy: 1.0000\n",
            "Epoch 47/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0910 - accuracy: 0.9968 - val_loss: 0.0773 - val_accuracy: 0.9994\n",
            "Epoch 48/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0906 - accuracy: 0.9965 - val_loss: 0.0767 - val_accuracy: 0.9994\n",
            "Epoch 49/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0920 - accuracy: 0.9952 - val_loss: 0.0772 - val_accuracy: 1.0000\n",
            "Epoch 50/100\n",
            "204/204 [==============================] - 1s 5ms/step - loss: 0.0919 - accuracy: 0.9969 - val_loss: 0.0751 - val_accuracy: 1.0000\n",
            "Epoch 51/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0945 - accuracy: 0.9951 - val_loss: 0.0762 - val_accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0922 - accuracy: 0.9957 - val_loss: 0.0774 - val_accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0881 - accuracy: 0.9969 - val_loss: 0.0723 - val_accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0896 - accuracy: 0.9963 - val_loss: 0.0733 - val_accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0891 - accuracy: 0.9962 - val_loss: 0.0775 - val_accuracy: 0.9994\n",
            "Epoch 56/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0877 - accuracy: 0.9969 - val_loss: 0.0716 - val_accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0887 - accuracy: 0.9948 - val_loss: 0.0747 - val_accuracy: 0.9994\n",
            "Epoch 58/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0874 - accuracy: 0.9974 - val_loss: 0.0725 - val_accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0865 - accuracy: 0.9971 - val_loss: 0.0715 - val_accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0845 - accuracy: 0.9980 - val_loss: 0.0704 - val_accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0868 - accuracy: 0.9968 - val_loss: 0.0726 - val_accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0865 - accuracy: 0.9962 - val_loss: 0.0716 - val_accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0872 - accuracy: 0.9963 - val_loss: 0.0717 - val_accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0879 - accuracy: 0.9966 - val_loss: 0.0708 - val_accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0848 - accuracy: 0.9969 - val_loss: 0.0722 - val_accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0831 - accuracy: 0.9966 - val_loss: 0.0681 - val_accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0835 - accuracy: 0.9968 - val_loss: 0.0721 - val_accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0836 - accuracy: 0.9972 - val_loss: 0.0697 - val_accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0839 - accuracy: 0.9968 - val_loss: 0.0690 - val_accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "204/204 [==============================] - 1s 4ms/step - loss: 0.0843 - accuracy: 0.9958 - val_loss: 0.0709 - val_accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0831 - accuracy: 0.9960 - val_loss: 0.0701 - val_accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0862 - accuracy: 0.9965 - val_loss: 0.0701 - val_accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0860 - accuracy: 0.9957 - val_loss: 0.0681 - val_accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0825 - accuracy: 0.9966 - val_loss: 0.0686 - val_accuracy: 0.9994\n",
            "Epoch 75/100\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0807 - accuracy: 0.9972 - val_loss: 0.0691 - val_accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "191/204 [===========================>..] - ETA: 0s - loss: 0.0874 - accuracy: 0.9954Restoring model weights from the end of the best epoch: 66.\n",
            "204/204 [==============================] - 1s 3ms/step - loss: 0.0871 - accuracy: 0.9957 - val_loss: 0.0722 - val_accuracy: 1.0000\n",
            "Epoch 76: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dropout rate of 0.5 after each dense layer. This means that during training, 50% of the neurons in each layer will be randomly disabled. This encourages the model to learn more robust and generalized features, which should help prevent overfitting."
      ],
      "metadata": {
        "id": "-bsNfaSod2Tt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Early stopping stops the training process when the modelâ€™s performance on the validation set starts to degrade. This helps prevent overfitting and allows us to choose the model with the best validation performance"
      ],
      "metadata": {
        "id": "I_u2danveMN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "AAQb6DSLhD5T",
        "outputId": "f047af42-cde2-4046-8a15-5caacde7e328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The modelâ€™s validation loss doesnâ€™t start to increase, and it follows the training loss much more closely, which suggests that the model is less overfit than before."
      ],
      "metadata": {
        "id": "chIxlhUbkvki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#save model as a HDF5 file\n",
        "model.save('mushroom_classification_model.h5')"
      ],
      "metadata": {
        "id": "QDEcXiBJloLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load model\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "loaded_model = load_model('mushroom_classification_model.h5')"
      ],
      "metadata": {
        "id": "Ju4pYSxgmQXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a new data point to use on saved model\n",
        "new_data = pd.DataFrame({\n",
        "    'cap-shape': ['f'],\n",
        "    'cap-surface': ['y'],\n",
        "    'cap-color': ['n'],\n",
        "    'bruises': ['f'],\n",
        "    'odor': ['a'],\n",
        "    'gill-attachment': ['f'],\n",
        "    'gill-spacing': ['c'],\n",
        "    'gill-size': ['n'],\n",
        "    'gill-color': ['b'],\n",
        "    'stalk-shape': ['e'],\n",
        "    'stalk-root': ['c'],\n",
        "    'stalk-surface-above-ring': ['s'],\n",
        "    'stalk-surface-below-ring': ['s'],\n",
        "    'stalk-color-above-ring': ['w'],\n",
        "    'stalk-color-below-ring': ['w'],\n",
        "    'veil-type': ['p'],\n",
        "    'veil-color': ['w'],\n",
        "    'ring-number': ['o'],\n",
        "    'ring-type': ['p'],\n",
        "    'spore-print-color': ['n'],\n",
        "    'population': ['c'],\n",
        "    'habitat': ['l']\n",
        "})"
      ],
      "metadata": {
        "id": "U9eqekgbmej3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess the data point\n",
        "new_data_encoded = pd.get_dummies(new_data)\n",
        "new_data_encoded = new_data_encoded.reindex(columns=X.columns, fill_value=0)"
      ],
      "metadata": {
        "id": "YxQiWQjzmzLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load model\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "loaded_model = load_model('mushroom_classification_model.h5')\n",
        "\n",
        "# Make predictions\n",
        "prediction = loaded_model.predict(new_data_encoded)\n",
        "prediction_class = int(prediction > 0.5)\n",
        "\n",
        "# Print the prediction\n",
        "print('Prediction:', prediction_class)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpsuCcf2qNFT",
        "outputId": "d80fd640-dd13-4f22-e59b-10b476cd469e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 77ms/step\n",
            "Prediction: 1\n"
          ]
        }
      ]
    }
  ]
}